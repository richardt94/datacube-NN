{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making tools to use covariate xarrays with a Keras model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading covariates and targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These should already exist in a bunch of pickle files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('site_and_points.pkl','rb') as f:\n",
    "    final_df = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"quantile_img.pkl\",\"rb\") as f:\n",
    "    quant_raster = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "#seasonal means\n",
    "with open(\"mean_seasonal_FC.pkl\",\"rb\") as f:\n",
    "    seasonal_means = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "PV_seas = seasonal_means.sel(variable='PV').rename({'season':'channel'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<xarray.DataArray (y: 886, x: 659, channel: 4)>\n",
       "array([[[61.97203 , 56.99014 , 60.784644, 50.163485],\n",
       "        [59.686388, 56.539571, 62.130111, 49.612824],\n",
       "        ...,\n",
       "        [52.804433, 45.763796, 51.412143, 43.88654 ],\n",
       "        [56.514986, 48.027604, 53.945511, 45.846932]],\n",
       "\n",
       "       [[59.696613, 57.998884, 62.081184, 50.653793],\n",
       "        [58.84326 , 54.444835, 58.516161, 50.029311],\n",
       "        ...,\n",
       "        [54.473558, 47.502116, 53.673721, 44.763188],\n",
       "        [57.525812, 51.180986, 58.85907 , 48.93493 ]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[74.482983, 52.336733, 62.816531, 67.265901],\n",
       "        [76.309705, 53.302597, 62.732599, 68.337951],\n",
       "        ...,\n",
       "        [59.712529, 59.333012, 70.1329  , 53.606898],\n",
       "        [55.674188, 54.775554, 69.946958, 49.202397]],\n",
       "\n",
       "       [[75.849694, 51.32138 , 62.854182, 68.617417],\n",
       "        [75.589411, 52.836265, 63.55757 , 65.914851],\n",
       "        ...,\n",
       "        [60.414193, 60.326271, 70.482568, 54.457972],\n",
       "        [58.453094, 58.396196, 70.109053, 51.88663 ]]])\n",
       "Coordinates:\n",
       "  * y        (y) float64 -3.697e+06 -3.697e+06 ... -3.719e+06 -3.719e+06\n",
       "  * x        (x) float64 1.782e+06 1.782e+06 1.782e+06 ... 1.798e+06 1.798e+06\n",
       "  * channel  (channel) object 'summer' 'winter' 'autumn' 'spring'"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PV_seas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<xarray.DataArray 'channel' (channel: 7)>\n",
       "array([0.01, 0.1 , 0.25, 0.5 , 0.75, 0.9 , 0.99])\n",
       "Coordinates:\n",
       "  * channel  (channel) float64 0.01 0.1 0.25 0.5 0.75 0.9 0.99"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quant_raster.channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "tpi = xr.open_rasterio('SOC_geotiff/TPI_ablers.tif')\n",
    "saga = xr.open_rasterio('SOC_geotiff/sagawetness_albers.tif')\n",
    "#need to make nodata non-numeric for standardisation to work correctly\n",
    "tpi = tpi.where(tpi!=tpi.nodatavals[0])\n",
    "saga = saga.where(saga!=saga.nodatavals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SampleID</th>\n",
       "      <th>Easting</th>\n",
       "      <th>Northing</th>\n",
       "      <th>TC</th>\n",
       "      <th>Method</th>\n",
       "      <th>Year</th>\n",
       "      <th>points</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2001_A1.2</td>\n",
       "      <td>338014.132</td>\n",
       "      <td>6370645.57</td>\n",
       "      <td>0.981252</td>\n",
       "      <td>CNS</td>\n",
       "      <td>2001</td>\n",
       "      <td>Geometry({'type': 'Point', 'coordinates': (178...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A1MIR</td>\n",
       "      <td>338014.132</td>\n",
       "      <td>6370645.57</td>\n",
       "      <td>0.600364</td>\n",
       "      <td>MIR</td>\n",
       "      <td>2001</td>\n",
       "      <td>Geometry({'type': 'Point', 'coordinates': (178...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2001_A6.2</td>\n",
       "      <td>338068.776</td>\n",
       "      <td>6370868.38</td>\n",
       "      <td>0.866419</td>\n",
       "      <td>CNS</td>\n",
       "      <td>2001</td>\n",
       "      <td>Geometry({'type': 'Point', 'coordinates': (178...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A6MIR</td>\n",
       "      <td>338068.776</td>\n",
       "      <td>6370868.38</td>\n",
       "      <td>1.187051</td>\n",
       "      <td>MIR</td>\n",
       "      <td>2001</td>\n",
       "      <td>Geometry({'type': 'Point', 'coordinates': (178...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2001_A11.2</td>\n",
       "      <td>338182.533</td>\n",
       "      <td>6370550.16</td>\n",
       "      <td>0.772519</td>\n",
       "      <td>CNS</td>\n",
       "      <td>2001</td>\n",
       "      <td>Geometry({'type': 'Point', 'coordinates': (178...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     SampleID     Easting    Northing        TC Method  Year  \\\n",
       "0   2001_A1.2  338014.132  6370645.57  0.981252    CNS  2001   \n",
       "1       A1MIR  338014.132  6370645.57  0.600364    MIR  2001   \n",
       "2   2001_A6.2  338068.776  6370868.38  0.866419    CNS  2001   \n",
       "3       A6MIR  338068.776  6370868.38  1.187051    MIR  2001   \n",
       "4  2001_A11.2  338182.533  6370550.16  0.772519    CNS  2001   \n",
       "\n",
       "                                              points  \n",
       "0  Geometry({'type': 'Point', 'coordinates': (178...  \n",
       "1  Geometry({'type': 'Point', 'coordinates': (178...  \n",
       "2  Geometry({'type': 'Point', 'coordinates': (178...  \n",
       "3  Geometry({'type': 'Point', 'coordinates': (178...  \n",
       "4  Geometry({'type': 'Point', 'coordinates': (178...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a DataFrame with all the position and target information about the site measurements, and raster maps of the TPI and SAGA wetness, along with quantiles of photosynthetic vegetation cover observed by Landsat. We should combine these separate rasters into one huge multi-channel raster, then write a function to select from this raster and produce a 'window' around a site measurement for input into the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "topographic = xr.concat((saga,tpi),dim='band').rename({'band':'channel'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 886, 659)"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topographic.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot rename 'quantile' because it is not a variable or dimension in this dataset",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-256-0fbca1fd935a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mquant_raster\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mquant_raster\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'quantile'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m'channel'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/g/data/v10/public/modules/dea-env/20190709/lib/python3.6/site-packages/xarray/core/dataarray.py\u001b[0m in \u001b[0;36mrename\u001b[0;34m(self, new_name_or_name_dict, **names)\u001b[0m\n\u001b[1;32m   1233\u001b[0m             name_dict = either_dict_or_kwargs(\n\u001b[1;32m   1234\u001b[0m                 new_name_or_name_dict, names, 'rename')\n\u001b[0;32m-> 1235\u001b[0;31m             \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_to_temp_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1236\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_from_temp_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1237\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/g/data/v10/public/modules/dea-env/20190709/lib/python3.6/site-packages/xarray/core/dataset.py\u001b[0m in \u001b[0;36mrename\u001b[0;34m(self, name_dict, inplace, **names)\u001b[0m\n\u001b[1;32m   2350\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdims\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2351\u001b[0m                 raise ValueError(\"cannot rename %r because it is not a \"\n\u001b[0;32m-> 2352\u001b[0;31m                                  \"variable or dimension in this dataset\" % k)\n\u001b[0m\u001b[1;32m   2353\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2354\u001b[0m         variables, coord_names, dims, indexes = self._rename_all(\n",
      "\u001b[0;31mValueError\u001b[0m: cannot rename 'quantile' because it is not a variable or dimension in this dataset"
     ]
    }
   ],
   "source": [
    "quant_raster = quant_raster.rename({'quantile':'channel'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "covars = xr.concat((topographic,quant_raster,PV_seas),dim='channel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove incompatible metadata in the dimension that was concatenated\n",
    "import numpy as np; covars['channel'] = np.arange(len(covars['channel']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<xarray.DataArray 'channel' (channel: 13)>\n",
       "array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12])\n",
       "Coordinates:\n",
       "  * channel  (channel) int64 0 1 2 3 4 5 6 7 8 9 10 11 12"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "covars.channel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to create training/validation samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function takes a point (x,y) and a specified buffer around it (in pixels), then returns a trimmed raster of the covariates around the point. It should deal with cases where the buffer zone intersects the edge of the covariate raster map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "#determine resolution of rasters by differencing the spatial dimensions.\n",
    "#ensure that if the raster contains covariates from different sources that they are coregistered\n",
    "#to the same spatial coordinate sets otherwise this won't work and you'll end up with a bunch of\n",
    "#NaNs in your underlying numpy arrays.\n",
    "xres = covars.x[1]-covars.x[0]\n",
    "yres = covars.y[1]-covars.y[0]\n",
    "\n",
    "def sample_raster(row,bufferx=10,buffery=10):\n",
    "    LL = row['points']\n",
    "    sitex = LL.coords[0][0]\n",
    "    sitey = LL.coords[0][1]\n",
    "    \n",
    "    x = np.arange(sitex-bufferx*xres,sitex+(bufferx+1)*xres,xres)\n",
    "    y = np.arange(sitey-buffery*yres,sitey+(buffery+1)*yres,yres)\n",
    "    \n",
    "    sample_array = covars.reindex(x=x,method='nearest',tolerance=abs(xres/2)).reindex(y=y,method='nearest',tolerance=abs(yres/2))\n",
    "    \n",
    "    sample_array = sample_array.fillna(0)\n",
    "    \n",
    "    return sample_array.data\n",
    "    \n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It may help to standardise the inputs for training. We can do this simply using built-in features of xarray before generating training samples. We can then either impute missing values (NaNs) on-the-fly using Keras or do it using our sample generating function while creating the training/validation set. It is less costly to do the latter because once it's done it will not need to be done again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "covars = (covars - covars.mean(dim=['x','y']))/covars.std(dim=['x','y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13, 886, 659)"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "covars.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the standardised covariate raster - this will come in handy later on\n",
    "with open(\"standardised_NN_covars.pkl\",\"wb\") as f:\n",
    "    pickle.dump(covars,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating labelled datasets for training and validation\n",
    "We can now iterate through the dataframe and save the input data associated with each sample site in a directory in the 'normal' way for use with a Keras generator. This avoids loading every sample into RAM to train the NN. The labels are the measured SOC values in the dataframe. We will need to associate each row of the dataframe with a unique file on disk which can be read by the generator which feeds samples to Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2183"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(final_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.981252\n",
       "1    0.600364\n",
       "2    0.866419\n",
       "3    1.187051\n",
       "4    0.772519\n",
       "5    1.398617\n",
       "6    0.593211\n",
       "7    1.126836\n",
       "8    1.315066\n",
       "9    1.881963\n",
       "Name: TC, dtype: float64"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df.iloc[0:10]['TC']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CovarGenerator(Sequence):\n",
    "    \"\"\"\n",
    "    Feed trimmed covariate images for an NN\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,gen_df,batch_size = 32, shuffle = True):\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        self.length = len(gen_df)//batch_size\n",
    "        \n",
    "        self.shuffle = shuffle\n",
    "        \n",
    "        if self.shuffle:\n",
    "            self.gen_df = gen_df.sample(frac=1).reset_index(drop=True)\n",
    "        else:\n",
    "            self.gen_df = gen_df\n",
    "        \n",
    "    def __getitem__(self,index):\n",
    "        slicedf = self.gen_df.iloc[index*self.batch_size:(index+1)*self.batch_size]\n",
    "        y = np.array(slicedf['TC'])\n",
    "        X = np.stack(slicedf.apply(sample_raster,axis=1))\n",
    "        #channels-last is default for TF+Keras\n",
    "        X = np.moveaxis(X,1,3)\n",
    "        return (X,y)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        if self.shuffle:\n",
    "            self.gen_df = self.gen_df.sample(frac=1).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "testgen = CovarGenerator(final_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y = testgen[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 21, 21, 13)"
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "68"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(testgen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32,)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = final_df.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = final_df.iloc[0:int(0.7*len(final_df))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_df = final_df.iloc[int(0.7*len(final_df)):int(0.85*len(final_df))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = final_df.iloc[int(0.85*len(final_df)):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_gen = CovarGenerator(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_gen = CovarGenerator(val_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CNN model\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv2D, Flatten, MaxPooling2D, SpatialDropout2D\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(8,kernel_size=10,activation='relu',input_shape=(21,21,13)))\n",
    "model.add(SpatialDropout2D(0.5))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1,activation='relu'))\n",
    "\n",
    "model.compile(optimizer='adam',loss='mse')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "47/47 [==============================] - 21s 449ms/step - loss: 5.1021 - val_loss: 2.2716\n",
      "Epoch 2/50\n",
      "47/47 [==============================] - 19s 411ms/step - loss: 3.9467 - val_loss: 2.3584\n",
      "Epoch 3/50\n",
      "47/47 [==============================] - 18s 389ms/step - loss: 3.5312 - val_loss: 1.8127\n",
      "Epoch 4/50\n",
      "47/47 [==============================] - 18s 390ms/step - loss: 3.1296 - val_loss: 2.5660\n",
      "Epoch 5/50\n",
      "47/47 [==============================] - 17s 369ms/step - loss: 3.3275 - val_loss: 2.0768\n",
      "Epoch 6/50\n",
      "47/47 [==============================] - 19s 402ms/step - loss: 2.9234 - val_loss: 2.3308\n",
      "Epoch 7/50\n",
      "47/47 [==============================] - 17s 357ms/step - loss: 3.0341 - val_loss: 2.1161\n",
      "Epoch 8/50\n",
      "47/47 [==============================] - 18s 381ms/step - loss: 2.7500 - val_loss: 2.2516\n",
      "Epoch 9/50\n",
      "47/47 [==============================] - 18s 386ms/step - loss: 2.8533 - val_loss: 2.2947\n",
      "Epoch 10/50\n",
      "47/47 [==============================] - 17s 371ms/step - loss: 3.1796 - val_loss: 1.9854\n",
      "Epoch 11/50\n",
      "47/47 [==============================] - 19s 411ms/step - loss: 3.0738 - val_loss: 2.8349\n",
      "Epoch 12/50\n",
      "47/47 [==============================] - 20s 416ms/step - loss: 2.7291 - val_loss: 1.7948\n",
      "Epoch 13/50\n",
      "47/47 [==============================] - 17s 363ms/step - loss: 2.5115 - val_loss: 1.9546\n",
      "Epoch 14/50\n",
      "47/47 [==============================] - 18s 382ms/step - loss: 2.3689 - val_loss: 1.9852\n",
      "Epoch 15/50\n",
      "47/47 [==============================] - 19s 402ms/step - loss: 2.2296 - val_loss: 1.8602\n",
      "Epoch 16/50\n",
      "47/47 [==============================] - 18s 382ms/step - loss: 2.1637 - val_loss: 1.8645\n",
      "Epoch 17/50\n",
      "47/47 [==============================] - 18s 380ms/step - loss: 2.2234 - val_loss: 1.6571\n",
      "Epoch 18/50\n",
      "47/47 [==============================] - 19s 399ms/step - loss: 2.0651 - val_loss: 1.6816\n",
      "Epoch 19/50\n",
      "47/47 [==============================] - 18s 387ms/step - loss: 2.1618 - val_loss: 1.8290\n",
      "Epoch 20/50\n",
      "47/47 [==============================] - 18s 393ms/step - loss: 2.1314 - val_loss: 1.7642\n",
      "Epoch 21/50\n",
      "47/47 [==============================] - 18s 383ms/step - loss: 2.0752 - val_loss: 1.7547\n",
      "Epoch 22/50\n",
      "47/47 [==============================] - 18s 391ms/step - loss: 1.9821 - val_loss: 1.7216\n",
      "Epoch 23/50\n",
      "47/47 [==============================] - 18s 390ms/step - loss: 1.9622 - val_loss: 1.8795\n",
      "Epoch 24/50\n",
      "47/47 [==============================] - 17s 370ms/step - loss: 1.9496 - val_loss: 1.5874\n",
      "Epoch 25/50\n",
      "47/47 [==============================] - 19s 395ms/step - loss: 1.8007 - val_loss: 1.5918\n",
      "Epoch 26/50\n",
      "47/47 [==============================] - 17s 357ms/step - loss: 1.9655 - val_loss: 1.5641\n",
      "Epoch 27/50\n",
      "47/47 [==============================] - 19s 413ms/step - loss: 1.7379 - val_loss: 1.8455\n",
      "Epoch 28/50\n",
      "47/47 [==============================] - 16s 340ms/step - loss: 1.8500 - val_loss: 1.8679\n",
      "Epoch 29/50\n",
      "47/47 [==============================] - 20s 421ms/step - loss: 1.8445 - val_loss: 1.8858\n",
      "Epoch 30/50\n",
      "47/47 [==============================] - 18s 373ms/step - loss: 1.8513 - val_loss: 1.6729\n",
      "Epoch 31/50\n",
      "47/47 [==============================] - 18s 374ms/step - loss: 1.6556 - val_loss: 1.5672\n",
      "Epoch 32/50\n",
      "47/47 [==============================] - 18s 389ms/step - loss: 1.6428 - val_loss: 1.6564\n",
      "Epoch 33/50\n",
      "47/47 [==============================] - 19s 399ms/step - loss: 1.6503 - val_loss: 1.5824\n",
      "Epoch 34/50\n",
      "47/47 [==============================] - 18s 373ms/step - loss: 1.7518 - val_loss: 1.7562\n",
      "Epoch 35/50\n",
      "47/47 [==============================] - 18s 385ms/step - loss: 1.5002 - val_loss: 1.6237\n",
      "Epoch 36/50\n",
      "47/47 [==============================] - 18s 384ms/step - loss: 1.5252 - val_loss: 1.6605\n",
      "Epoch 37/50\n",
      "47/47 [==============================] - 19s 406ms/step - loss: 1.4912 - val_loss: 1.6492\n",
      "Epoch 38/50\n",
      "47/47 [==============================] - 18s 389ms/step - loss: 1.4952 - val_loss: 1.6357\n",
      "Epoch 39/50\n",
      "47/47 [==============================] - 17s 358ms/step - loss: 1.6266 - val_loss: 1.6614\n",
      "Epoch 40/50\n",
      "47/47 [==============================] - 19s 394ms/step - loss: 1.5089 - val_loss: 1.5749\n",
      "Epoch 41/50\n",
      "47/47 [==============================] - 18s 373ms/step - loss: 1.4425 - val_loss: 1.6181\n",
      "Epoch 42/50\n",
      "47/47 [==============================] - 18s 381ms/step - loss: 1.6187 - val_loss: 1.6590\n",
      "Epoch 43/50\n",
      "47/47 [==============================] - 17s 351ms/step - loss: 1.4719 - val_loss: 1.6156\n",
      "Epoch 44/50\n",
      "47/47 [==============================] - 18s 390ms/step - loss: 1.5040 - val_loss: 1.6842\n",
      "Epoch 45/50\n",
      "47/47 [==============================] - 17s 357ms/step - loss: 1.6279 - val_loss: 1.6699\n",
      "Epoch 46/50\n",
      "47/47 [==============================] - 19s 404ms/step - loss: 1.4763 - val_loss: 1.5183\n",
      "Epoch 47/50\n",
      "47/47 [==============================] - 18s 386ms/step - loss: 1.4967 - val_loss: 1.7040\n",
      "Epoch 48/50\n",
      "47/47 [==============================] - 18s 376ms/step - loss: 1.4401 - val_loss: 1.6481\n",
      "Epoch 49/50\n",
      "47/47 [==============================] - 17s 366ms/step - loss: 1.3186 - val_loss: 1.5162\n",
      "Epoch 50/50\n",
      "47/47 [==============================] - 18s 391ms/step - loss: 1.4346 - val_loss: 1.6488\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f6b28d26588>"
      ]
     },
     "execution_count": 332,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(training_gen,epochs=50,validation_data=validation_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.9286025583744049"
      ]
     },
     "execution_count": 333,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testgen = CovarGenerator(test_df)\n",
    "model.evaluate_generator(testgen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict_generator(testgen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsoc = 0\n",
    "for batch,soc in training_gen:\n",
    "    tsoc += np.mean(soc)\n",
    "msoc = tsoc/len(training_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.5734063676475403"
      ]
     },
     "execution_count": 289,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "msoc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'conv2d_44/kernel:0' shape=(5, 5, 13, 8) dtype=float32>"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.weights[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<xarray.DataArray (channel: 13)>\n",
       "array([ 1.619998e-16,  3.902013e-16,  7.919880e-16,  9.558375e-16,\n",
       "       -6.707803e-17, -2.570189e-17,  2.267899e-16, -1.490125e-15,\n",
       "        8.933352e-16,  5.919222e-17, -5.705040e-16,  6.489726e-16,\n",
       "       -1.007047e-15])\n",
       "Coordinates:\n",
       "  * channel  (channel) int64 0 1 2 3 4 5 6 7 8 9 10 11 12"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "covars.mean(dim=['x','y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<xarray.DataArray (channel: 13)>\n",
       "array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
       "Coordinates:\n",
       "  * channel  (channel) int64 0 1 2 3 4 5 6 7 8 9 10 11 12"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "covars.std(dim=['x','y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<xarray.DataArray (channel: 13)>\n",
       "array([-0.037357, -0.243832,  0.477716,  0.557435,  0.268647,  0.097549,\n",
       "       -0.154819, -0.255622, -0.324444,  0.154503,  0.104352,  0.116006,\n",
       "       -0.331155])\n",
       "Coordinates:\n",
       "    y        float64 -3.704e+06\n",
       "    x        float64 1.789e+06\n",
       "  * channel  (channel) int64 0 1 2 3 4 5 6 7 8 9 10 11 12"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "covars.isel(x=300,y=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def sample_dc(row, dc, bufferx = 10, buffery = 10, xres = 25, yres = -25, product = 'fc_percentile_albers_annual', crs = 'EPSG:3577', **query):\n",
    "    LL = row['points']\n",
    "    sitex = LL.coords[0][0]\n",
    "    sitey = LL.coords[0][1]\n",
    "    \n",
    "    x = np.arange(sitex-bufferx*xres,sitex+(bufferx+1)*xres,xres)\n",
    "    y = np.arange(sitey-buffery*yres,sitey+(buffery+1)*yres,yres)\n",
    "    \n",
    "    covars = dc.load(product = product, x = (min((x[0],x[-1])),max((x[0],x[-1]))), y = (min((y[0],y[-1])),max((y[0],y[-1]))), crs = crs, **query)\n",
    "    \n",
    "    print(len(covars.variables))\n",
    "    \n",
    "    sample_array = covars.reindex(x=x,method='nearest',tolerance=abs(xres/2)).reindex(y=y,method='nearest',tolerance=abs(yres/2))\n",
    "    \n",
    "    sample_array = sample_array.fillna(0)\n",
    "    \n",
    "    return sample_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datacube\n",
    "dc = datacube.Datacube()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "trow = final_df.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n"
     ]
    }
   ],
   "source": [
    "test_xr = sample_dc(trow,dc).isel(time=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "mband_np = test_xr.to_array().data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sitex,sitey = trow['points'].coords[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1784613.2479390604"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sitex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-3710538.5739176488"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sitey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = {\n",
    "    'x': (sitex-300,sitex+300),\n",
    "    'y': (sitey-300,sitey+300),\n",
    "    'time': ('2001-03-01','2002-03-01')\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<xarray.Dataset>\n",
       "Dimensions:    (time: 1, x: 25, y: 25)\n",
       "Coordinates:\n",
       "  * time       (time) datetime64[ns] 2002-01-01\n",
       "  * y          (y) float64 -3.71e+06 -3.71e+06 ... -3.711e+06 -3.711e+06\n",
       "  * x          (x) float64 1.784e+06 1.784e+06 1.784e+06 ... 1.785e+06 1.785e+06\n",
       "Data variables:\n",
       "    BS_PC_10   (time, y, x) int8 0 0 4 18 19 22 19 7 4 4 ... 7 8 6 8 0 4 2 0 0 8\n",
       "    PV_PC_10   (time, y, x) int8 47 33 19 10 8 8 13 33 ... 30 26 31 37 39 40 37\n",
       "    NPV_PC_10  (time, y, x) int8 25 36 45 43 42 38 41 ... 21 14 18 17 21 19 17\n",
       "    BS_PC_50   (time, y, x) int8 1 7 19 36 38 39 33 13 8 ... 9 13 11 8 7 6 8 18\n",
       "    PV_PC_50   (time, y, x) int8 61 47 34 14 13 14 22 ... 38 41 40 44 50 52 47\n",
       "    NPV_PC_50  (time, y, x) int8 37 44 48 49 47 46 44 ... 46 47 47 43 41 42 36\n",
       "    BS_PC_90   (time, y, x) int8 6 17 31 45 44 48 42 23 ... 17 16 13 11 11 11 20\n",
       "    PV_PC_90   (time, y, x) int8 70 57 51 29 27 21 32 ... 75 83 81 81 76 79 72\n",
       "    NPV_PC_90  (time, y, x) int8 45 49 53 52 54 55 50 ... 53 57 56 54 50 50 43\n",
       "Attributes:\n",
       "    crs:      EPSG:3577"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dc.load(product = 'fc_percentile_albers_annual',**query, crs='EPSG:3577')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "prod = dc.list_products()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id\n",
       "36                                             EPSG:4326\n",
       "88                                             EPSG:3577\n",
       "32                                             EPSG:4326\n",
       "99                                             EPSG:3577\n",
       "53     GEOGCS[\"GEOCENTRIC DATUM of AUSTRALIA\",DATUM[\"...\n",
       "                             ...                        \n",
       "97                                             EPSG:3577\n",
       "100                                            EPSG:3577\n",
       "85                                             EPSG:3577\n",
       "101                                            EPSG:3577\n",
       "84                                             EPSG:3577\n",
       "Name: crs, Length: 77, dtype: object"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prod['crs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Geometry(POINT (1784613.24793906 -3710538.57391765), EPSG:3577)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trow['points']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "prod = dc.list_products()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('y', 'x')"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prod[prod['name']=='fc_percentile_albers_annual']['spatial_dimensions'].array[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-300., -275., -250., -225., -200., -175., -150., -125., -100.,\n",
       "        -75.,  -50.,  -25.,    0.])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.arange(-300.,25.,25.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Geometry(POINT (151.270161902695 -32.7902091714388), EPSG:4326)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trow['points'].to_crs(CRS('EPSG:4326'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datacube.utils.geometry import CRS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tifffile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tifffile.imsave('testTIF.tif',data=mband_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Geometry(POINT (1784613.24793906 -3710538.57391765), EPSG:3577)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df['points'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "attempted relative import beyond top-level package",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-90a0d5ea2977>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatacube_NN\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmodel_development\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m: attempted relative import beyond top-level package"
     ]
    }
   ],
   "source": [
    "from ..datacube_NN import model_development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
